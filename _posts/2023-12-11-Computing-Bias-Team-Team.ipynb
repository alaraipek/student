{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "toc: true\n",
    "comments: false\n",
    "layout: post\n",
    "title: Computing Bias Team Teach\n",
    "description: Computing Bias Team Teach\n",
    "type: tangibles\n",
    "courses: { compsci: {week: 11} }\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Computing Bias\n",
    "## Popcorn Hack #1: \n",
    "D is the correct answer\n",
    "## Popcorn Hack #2: \n",
    "This situation is most likely a form of unconscious data bias. It's likely an unintentional consequence of mostly testing and gathering data on facial recognition with people of lighter skin than the person who wasn't recognized by the technology.\n",
    "## Popcorn Hack #3: \n",
    "Moving with a mobile phone or another device involves the gathering of data, including various types such as location information obtained through tracking.\n",
    "\n",
    "# Homework Hack #4: \n",
    "I would utilize Output Correction as a strategy to tackle the issue through mitigation. This involves identifying and correcting instances where the algorithm unfairly focused on specific neighborhoods. In essence, it requires adjusting the algorithm's resource allocation decisions to achieve a more equitable distribution of law enforcement across various neighborhoods. The ultimate objective is to eliminate systemic biases, preventing over-policing in particular areas and promoting a more balanced approach to law enforcement actions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
